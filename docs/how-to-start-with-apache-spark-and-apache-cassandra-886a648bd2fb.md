# å¦‚ä½•ä» Apache Spark å’Œ Apache Cassandra å¼€å§‹

> åŸæ–‡ï¼š<https://itnext.io/how-to-start-with-apache-spark-and-apache-cassandra-886a648bd2fb?source=collection_archive---------0----------------------->

![](img/bbacd483a7834e0b96648c8e38d085b7.png)

Apache Cassandra æ˜¯ä¸€ä¸ªç‰¹å®šçš„æ•°æ®åº“ï¼Œå¯ä»¥çº¿æ€§æ‰©å±•ã€‚è¿™æ˜¯æœ‰ä»£ä»·çš„:ç‰¹å®šçš„è¡¨æ ¼æ¨¡å‹ã€å¯é…ç½®çš„ä¸€è‡´æ€§å’Œæœ‰é™çš„åˆ†æã€‚è‹¹æœæ¯ç§’åœ¨è¶…è¿‡ 160ï¼Œ000 ä¸ª Cassandra å®ä¾‹ä¸Šæ‰§è¡Œæ•°ç™¾ä¸‡æ¬¡æ“ä½œï¼ŒåŒæ—¶æ”¶é›†è¶…è¿‡ 100 PBs çš„æ•°æ®ã€‚æ‚¨å¯ä»¥é€šè¿‡ Apache Spark å’Œ DataStax è¿æ¥å™¨ç»•è¿‡è¿™äº›æœ‰é™çš„åˆ†æï¼Œè¿™å°±æ˜¯æœ¬æ–‡çš„å†…å®¹ã€‚

# è®¾ç½®

æˆ‘åœ¨ Docker ä¸Šä½¿ç”¨è¿‡ä¸€ä¸ª Apache Cassandra èŠ‚ç‚¹

```
version: '3'

services:
  cassandra:
    image: cassandra:latest
    ports:
      - "9042:9042"
```

Apache Spark 3.0 ä½œä¸º shell æ¨å‡ºï¼Œå¸¦æœ‰è¿æ¥å™¨å’Œ Cassandra çš„å®¢æˆ·ç«¯åº“ï¼Œè¿™å¯¹äº timeuuid ç±»å‹è½¬æ¢å°†å¾ˆæœ‰ç”¨ã€‚

```
./spark-shell --packages com.datastax.spark:spark-cassandra-connector_2.12:3.0.0-beta,com.datastax.cassandra:cassandra-driver-core:3.9.0
```

å¦‚æœ Cassandra æ²¡æœ‰åœ¨æœ¬åœ°è¿è¡Œï¼Œæ‚¨éœ€è¦é…ç½®å®ƒçš„åœ°å€ã€‚

```
spark.conf.set("spark.cassandra.connection.host", "127.0.0.1")
```

# æ•°æ®

ä¸ºäº†æµ‹è¯• Spark + Cassandra ç»„åˆï¼Œæˆ‘ä½¿ç”¨ mockaroo.com ç”Ÿæˆäº†ä¸€äº›æ—¥æœŸã€‚è¿™æ˜¯ä¸€ä¸ªä¼ æ„Ÿå™¨åˆ—è¡¨å’Œè¿™äº›ä¼ æ„Ÿå™¨çš„æµ‹é‡å€¼åˆ—è¡¨ã€‚ä½ å¯ä»¥åœ¨ GitHub çš„[åº“](https://github.com/zorteran/wiadro-danych-spark-cassandra-101)ä¸­æ‰¾åˆ°å®ƒä»¬ã€‚

```
maciej@ubuntu:~/Desktop/spark_and_cassandra$ head sensor_reads.csv 
date,sensor_id,temperature,wind_speed,wind_direction
2020-02-20 13:00:57,11,90.42,72.91,153
2020-05-28 21:31:03,9,51.62,20.07,255
2020-06-04 16:32:02,3,6.68,89.31,309
...
```

# åœ¨ Apache Cassandra ä¸­åˆ›å»ºè¡¨æ ¼

Apache Cassandra æœ‰ä¸€ä¸ªå«åš *cqlsh* çš„ä¸“ç”¨å·¥å…·ã€‚åœ¨ Docker çš„æƒ…å†µä¸‹ï¼Œæ‚¨å¿…é¡»ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ã€‚

```
sudo docker exec -it container_name cqlsh
```

é¦–å…ˆï¼Œæˆ‘ä»¬å¿…é¡»åˆ›å»ºä¸€ä¸ª keyspaceï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨æ¥å­˜æ”¾æˆ‘ä»¬çš„è¡¨çš„åŒ…ã€‚

```
**CREATE** KEYSPACE spark_playground **WITH** replication = {'class': 'SimpleStrategy', 'replication_factor': 1 };
```

æœ‰å…³ä¼ æ„Ÿå™¨çš„ä¿¡æ¯å°†ä¿å­˜åœ¨ä¼ æ„Ÿå™¨è¡¨ä¸­ã€‚æ²¡ä»€ä¹ˆå¼‚å¸¸:idï¼Œä½ç½®ï¼Œç»„ idã€‚æ³¨æ„ï¼Œä¸»é”®æ˜¯ idã€‚å› æ­¤ï¼Œåœ¨æ²¡æœ‰å®Œæ•´çš„è¡¨æ‰«æçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸ä¼šå¯¹å…¶ä»–åˆ—æ‰§è¡Œ WHERE æ“ä½œ(æˆ‘æåˆ°å…·ä½“çš„æ•°æ®å»ºæ¨¡äº†å—ï¼Ÿ)ï¼Œä½†æ˜¯æŒ‰é”®è¿‡æ»¤ä¼šè¶…çº§å¿«ã€‚

```
**CREATE** **TABLE** sensors ( sensor_id **int**, location text, group_id **int**, **PRIMARY** **KEY** (sensor_id ));
```

æµ‹é‡å€¼å°†åœ¨ sensors_reads è¡¨ä¸­ã€‚å¯†é’¥ç”±ä¼ æ„Ÿå™¨ id(åˆ†åŒºå¯†é’¥)å’Œæ—¥æœŸç»„æˆ [timeuuid](https://docs.datastax.com/en/cql-oss/3.3/cql/cql_reference/uuid_type_r.html) (èšç±»å¯†é’¥)ã€‚åˆ†åŒºé”®æŒ‡ç¤ºè®°å½•çš„ä½ç½®(åœ¨å“ªä¸ªèŠ‚ç‚¹ä¸­)ã€‚Timeuuid ä½œä¸ºä¸€ä¸ªèšç±»é”®å…è®¸å¯¹è®°å½•è¿›è¡Œæ’åºã€‚å¦‚æœä½ è¿·è·¯äº†ï¼Œçœ‹çœ‹[å…³äº](https://stackoverflow.com/questions/24949676/difference-between-partition-key-composite-key-and-clustering-key-in-cassandra)æ ˆæº¢å‡ºçš„è§£é‡Šã€‚æ‚¨å¿…é¡»å°å¿ƒé€‰æ‹©åˆ†åŒºé”®ã€‚**é”™è¯¯çš„é€‰æ‹©ä¼šå¯¼è‡´èŠ‚ç‚¹ä¸Šçš„è´Ÿè½½ä¸å‡åŒ€**ã€‚

```
**CREATE** **TABLE** sensors_reads ( sensor_id **int**, **date** timeuuid, **temp** **double**, humidity **double**, wind_speed **double**, wind_direction **double**, **PRIMARY** **KEY** (sensor_id, **date** ));
```

# ä½¿ç”¨ Spark å‘ Cassandra æ·»åŠ æ•°æ®

## åŠ è½½ CSV æ–‡ä»¶

```
val sensors = spark.read.format("csv").option("header", "true").load("/home/maciej/Desktop/spark_and_cassandra/sensors.csv")
val sensorReads = spark.read.format("csv").option("header", "true").load("/home/maciej/Desktop/spark_and_cassandra/sensor_reads.csv")
```

## å†™å…¥ä¼ æ„Ÿå™¨è¡¨

åˆ—çš„ç±»å‹å’Œåç§°éƒ½ä¸€è‡´ï¼Œæ‰€ä»¥æ“ä½œå¾ˆç®€å•ã€‚

```
sensors.write
    .format("org.apache.spark.sql.cassandra")
    .option("keyspace","spark_playground")
    .option("table","sensors")
    .mode("append")
    .save()
```

## å†™å…¥ä¼ æ„Ÿå™¨è¯»å–è¡¨

è¿™é‡Œå˜å¾—æ›´åŠ å¤æ‚:

*   æ¸©åº¦åˆ—ä¸­çš„åˆ—åä¸ä¸€è‡´ã€‚æ¸©åº¦= >æ¸©åº¦
*   æ—¥æœŸåˆ—ä¸­çš„ç±»å‹ä¸ä¸€è‡´ã€‚æˆ‘ä»¬ä» CSV ä¸­è¯»å–å­—ç¬¦ä¸²å½¢å¼çš„æ—¥æœŸï¼Œè€Œ Cassandra ä¸­çš„åˆ—ç±»å‹æ˜¯ timeuuidã€‚æˆ‘ä»¬éœ€è¦ä½¿ç”¨ Cassandra çš„å®¢æˆ·ç«¯åº“æ¥è¿›è¡Œè½¬æ¢ã€‚

ç±»å‹ä¸ä¸€è‡´çš„é—®é¢˜å¯ä»¥é€šè¿‡é€‚å½“çš„ UDF(ç”¨æˆ·å®šä¹‰çš„å‡½æ•°)æ¥è§£å†³ã€‚é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œæˆ‘ä»¬å°†æ£€æŸ¥åå‘å‡½æ•°æ˜¯å¦è¿”å›ç›¸åŒçš„æ—¥æœŸã€‚é€šå¸¸æˆ‘ä¼šç”¨ IntelliJ ç¼–å†™ä¸€ä¸ªæµ‹è¯•ï¼Œä½†æ˜¯è®©æˆ‘ä»¬ç”¨ spark-shell çš„æ–¹å¼æ¥åšğŸ˜Šã€‚

```
import spark.implicits._
import com.datastax.driver.core.utils.UUIDs
import org.apache.spark.sql.functions.udf

val toTimeuuid: java.sql.Timestamp => String = x => UUIDs.startOf(x.getTime()).toString()
val fromTimeuuid: String => java.sql.Timestamp = x => new java.sql.Timestamp(UUIDs.unixTimestamp(java.util.UUID.fromString(x)))

val toTimeuuidUDF = udf(toTimeuuid)
val fromTimeuuidUDF = udf(fromTimeuuid)

sensorsReads
    .withColumn("date_as_timestamp", to_timestamp($"date"))
    .withColumn("date_as_timeuuid", toTimeuuidUDF($"date_as_timestamp"))
    .withColumn("timestamp_from_timeuuid",fromTimeuuidUDF($"date_as_timeuuid"))
    .show(false)
```

![](img/25e2141eca7f9fc1ea0b1e688c73d535.png)

çœ‹èµ·æ¥ä¸é”™ã€‚

ç°åœ¨æˆ‘ä»¬å»æ‰ä¸å¿…è¦çš„åˆ—ï¼Œé‡å‘½åæ¸©åº¦åˆ—ï¼Œå¹¶å°†æ•°æ®ä¿å­˜åœ¨ Cassandra ä¸­

```
val sensorsReads_fixed = sensorsReads
                            .withColumn("date_as_timestamp", to_timestamp($"date"))
                            .withColumn("date_as_timeuuid", toTimeuuidUDF($"date_as_timestamp"))
                            .drop("date").drop("date_as_timestamp")
                            .withColumnRenamed("date_as_timeuuid","date")
                            .withColumnRenamed("temperature","temp")
sensorsReads_fixed.write
    .format("org.apache.spark.sql.cassandra")
    .option("keyspace","spark_playground")
    .option("table","sensors_reads")
    .mode("append")
    .save()
```

# é˜…è¯»å¡çŠå¾·æ‹‰çš„ä½œå“

åœ¨ Cassandra ä¸­å¯ä»¥ç®€åŒ–å¯¹è¡¨çš„å¼•ç”¨ã€‚æ‚¨åªéœ€è¦é…ç½® Spark ä¸Šä¸‹æ–‡ã€‚

```
spark.conf.set("spark.sql.catalog.casscatalog","com.datastax.spark.connector.datasource.CassandraCatalog")
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥ä½¿ç”¨*cas catalog . key space _ name . table _ name*ç¬¦å·æ¥å¼•ç”¨è¿™äº›è¡¨ã€‚

```
scala> spark.read.table("casscatalog.spark_playground.sensors").show()
+---------+--------+--------------------+
|sensor_id|group_id|            location|
+---------+--------+--------------------+
|        2|       4|       027 Heath Way|
|       13|       3|  42585 Ramsey Alley|
|        4|       1|     676 Marcy Point|
|        5|       3|260 Steensland Cr...|
|        9|       3|9385 Comanche Ter...|
|        8|       2|291 Meadow Ridge ...|
|        7|       2|     716 Randy Point|
|       14|       2|    331 Mcbride Road|
|       15|       3|     91 Gateway Hill|
|        1|       3|      66 Vera Avenue|
|        6|       4|87212 Lake View S...|
|       12|       2|    12 Montana Place|
|       10|       3|      60 Spohn Plaza|
|       11|       1|    48 Redwing Court|
|        3|       3|        930 Almo Way|
+---------+--------+--------------------+**val** sensors**_**table **=** spark.read.table("casscatalog.spark_playground.sensors")**val** sensors**_**reads**_**table **=** spark.read.table("casscatalog.spark_playground.sensors_reads")
```

## æ‰‹æœ¯è´¹ç”¨

æœ‰äº† Sparkï¼Œä¸€åˆ‡éƒ½æ˜¾å¾—è½»æ¾æ„‰å¿«ã€‚æˆ‘ä»¬å¿…é¡»è®°ä½ï¼Œæˆ‘ä»¬æ“ä½œçš„æ˜¯ Cassandraï¼Œå®ƒæœ‰è‡ªå·±å¤„ç†æŸ¥è¯¢çš„æ–¹å¼ã€‚æˆ‘ä¸»è¦å…³å¿ƒé€šè¿‡é”®å’Œå€¼æ£€ç´¢è®°å½•çš„é€Ÿåº¦ã€‚

```
scala> sensors_table.filter("sensor_id in (1,2,3)").select("sensor_id","location").explain
20/07/21 11:09:50 INFO V2ScanRelationPushDown:
Pushing operators to sensors
Pushed Filters: In(sensor_id, [1,2,3])
Post-Scan Filters:
Output: sensor_id#749, location#751

== Physical Plan ==
*(1) Project [sensor_id#749, location#751]
+- BatchScan[sensor_id#749, location#751] Cassandra Scan: spark_playground.sensors
 - Cassandra Filters: [["sensor_id" IN (?, ?, ?), 1]]
 - Requested Columns: [sensor_id,location]
```

é€šè¿‡ sensor_id è¿‡æ»¤å‘ç”Ÿåœ¨ä» Cassandra æ£€ç´¢æ•°æ®çš„é˜¶æ®µã€‚æŒ‰ group_id è¿‡æ»¤éœ€è¦æŒ‰ Spark æ‰«ææ•´ä¸ªè¡¨ã€‚

```
scala> sensors_table.filter("group_id in (1,2,3)").select("sensor_id","location").explain
20/07/21 11:14:34 INFO V2ScanRelationPushDown:
Pushing operators to sensors
Pushed Filters:
Post-Scan Filters: group_id#750 IN (1,2,3)
Output: sensor_id#749, group_id#750, location#751

== Physical Plan ==
*(1) Project [sensor_id#749, location#751]
+- *(1) Filter group_id#750 IN (1,2,3)
   +- BatchScan[sensor_id#749, group_id#750, location#751] Cassandra Scan: spark_playground.sensors
 - Cassandra Filters: []
 - Requested Columns: [sensor_id,group_id,location]
```

## ç®€å•èšåˆ

è®©æˆ‘ä»¬å‡è®¾éœ€è¦è®¡ç®—ä¼ æ„Ÿå™¨ç»„çš„æ•°é‡ã€‚æˆ‘ä»¬æ²¡æœ‰åœ¨æ•°æ®åº“è®¾è®¡çº§åˆ«é¢„æµ‹åˆ°è¿™ä¸€ç‚¹ï¼ŒCQL æŸ¥è¯¢ä¹Ÿä¸ä¼šè¢«æ‰§è¡Œã€‚

```
cqlsh:spark_playground> SELECT group_id, count(1) FROM sensors GROUP BY group_id;
InvalidRequest: Error from server: code=2200 [Invalid query] message="Group by is currently only supported on the columns of the PRIMARY KEY, got group_id"
```

å› æ­¤ï¼Œæˆ‘ä»¬è¦ä¹ˆåœ¨å®¢æˆ·ç«¯åº”ç”¨ç¨‹åºç«¯è¿›è¡Œï¼Œè¦ä¹ˆä½¿ç”¨ Apache Sparkã€‚

```
scala> sensors_table.groupBy("group_id").count.show
+--------+-----+                                                                
|group_id|count|
+--------+-----+
|       1|    2|
|       3|    7|
|       4|    2|
|       2|    4|
+--------+-----+
```

## è¿æ¥ joinWithCassandraTable

æœ€ç®€å•çš„è¿æ¥æ–¹æ³•æ˜¯å–ä¸¤ä¸ªé›†åˆï¼Œåšä¸€ä¸ªç¬›å¡å°”ç§¯ã€‚ç„¶è€Œï¼ŒCassandra è¿æ¥å™¨æä¾›äº†ä¸€ä¸ªæ›´å¿«çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨ RDD ç‰ˆæœ¬ä¸­æœ‰ä¸€ä¸ª[joinwithcassandrable](https://github.com/datastax/spark-cassandra-connector/blob/master/doc/2_loading.md#using-joinwithcassandratable)æ–¹æ³•ï¼Œè€Œåœ¨æ•°æ®æ¡†æ¶ä¸­æœ‰ [Direct Join](https://github.com/datastax/spark-cassandra-connector/blob/master/doc/14_data_frames.md#direct-join) ï¼Œæ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œå®ƒçš„æ–‡æ¡£å¾ˆå·®ã€‚joinWithCassandraTable ä¸ºæº RDD æ‰€éœ€çš„æ¯ä¸ªåˆ†åŒºæ‰§è¡Œä¸€æ¬¡æŸ¥è¯¢ã€‚åœ¨æ•°æ®å¸§çš„æƒ…å†µä¸‹ï¼Œè¿™æ˜¯è‡ªåŠ¨å‘ç”Ÿçš„ã€‚

```
scala> sensors_reads_table.join(sensors_table).explain
...    
== Physical Plan ==
CartesianProduct
:- *(1) Project [date#755, sensor_id#756, humidity#757, temp#758, wind_direction#759, wind_speed#760]
:  +- BatchScan[date#755, sensor_id#756, humidity#757, temp#758, wind_direction#759, wind_speed#760] Cassandra Scan: spark_playground.sensors_reads
 - Cassandra Filters: []
 - Requested Columns: [date,sensor_id,humidity,temp,wind_direction,wind_speed]
+- *(2) Project [sensor_id#749, group_id#750, location#751]
   +- BatchScan[sensor_id#749, group_id#750, location#751] Cassandra Scan: spark_playground.sensors
 - Cassandra Filters: []
 - Requested Columns: [sensor_id,group_id,location]
```

ç†è®ºä¸Šï¼Œç›¸åŒçš„æ“ä½œï¼Œä½†æ˜¯é€‰æ‹©é”®æ§å…³èŠ‚ä¼šç”Ÿæˆæ›´æœ‰æ•ˆçš„æ‰§è¡Œè®¡åˆ’ã€‚

```
scala> sensors_reads_table.join(sensors_table, sensors_reads_table("sensor_id") === sensors_table("sensor_id"), "inner").explain
...
== Physical Plan ==
*(5) SortMergeJoin [sensor_id#756], [sensor_id#749], Inner
:- *(2) Sort [sensor_id#756 ASC NULLS FIRST], false, 0
:  +- Exchange hashpartitioning(sensor_id#756, 200), true, [id=#812]
:     +- *(1) Project [date#755, sensor_id#756, humidity#757, temp#758, wind_direction#759, wind_speed#760]
:        +- BatchScan[date#755, sensor_id#756, humidity#757, temp#758, wind_direction#759, wind_speed#760] Cassandra Scan: spark_playground.sensors_reads
 - Cassandra Filters: []
 - Requested Columns: [date,sensor_id,humidity,temp,wind_direction,wind_speed]
+- *(4) Sort [sensor_id#749 ASC NULLS FIRST], false, 0
   +- Exchange hashpartitioning(sensor_id#749, 200), true, [id=#820]
      +- *(3) Project [sensor_id#749, group_id#750, location#751]
         +- BatchScan[sensor_id#749, group_id#750, location#751] Cassandra Scan: spark_playground.sensors
 - Cassandra Filters: []
 - Requested Columns: [sensor_id,group_id,location]
```

# ç¼–è¾‘(2020 å¹´ 7 æœˆ 30 æ—¥)

Datastax çš„ Alex Ott åœ¨ Linkedin çš„è¯„è®ºä¸­å‘Šè¯‰æˆ‘ï¼Œåœ¨è¿™ä¸ªæ¡ˆä¾‹ä¸­æ²¡æœ‰ç›´æ¥åŠ å…¥ã€‚åŸæ¥ä½ éœ€è¦é…ç½® spark.sql.extensionsï¼Œæ›´å¤šè¯¦æƒ…å¯ä»¥åœ¨è¿™é‡Œ[æ‰¾åˆ°](https://www.datastax.com/blog/2020/05/advanced-apache-cassandra-analytics-now-open-all)ã€‚

```
spark.conf.set("spark.sql.extensions", "com.datastax.spark.connector.CassandraSparkExtensions")
```

# è´®è—å®¤ËŒä»“åº“

[https://github . com/zorteran/wiadro-danych-spark-Cassandra-101](https://github.com/zorteran/wiadro-danych-spark-cassandra-101)â€”docker-compose å’Œ CSV

# æ‘˜è¦

Spark å’Œ Cassandra åˆä½œçš„è¯é¢˜åœ¨è¿™ä¸ªè¯æ¡ä¸­å‡ ä¹æ²¡æœ‰æåŠã€‚Cassandra æ˜¯ Hadoop ç”Ÿæ€ç³»ç»Ÿçš„ä¸€ä¸ªæœ‰è¶£çš„æ›¿ä»£å’Œ/æˆ–è¡¥å……ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Spark è¿›è¡Œåˆ†æï¼Œä¹Ÿå¯ä»¥ç»´æŠ¤å’Œé›†æˆ Cassandra çš„æ•°æ®ã€‚æ¯•ç«Ÿï¼Œåœ¨ç¬¬ä¸€ç§æ–¹æ³•ä¸­å¾ˆéš¾æ‰¾åˆ°ç†æƒ³çš„æ•°æ®æ¨¡å‹ğŸ˜‰ã€‚