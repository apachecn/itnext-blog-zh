# å¦‚ä½•è®¾ç½®æœ¬åœ° Apache Spark ç¯å¢ƒ(5 ç§æ–¹æ³•)

> åŸæ–‡ï¼š<https://itnext.io/how-to-set-up-local-apache-spark-environment-5-ways-62910fa0e8ad?source=collection_archive---------0----------------------->

![](img/e9fd2401d5e921b7f9ae4b19685248e3.png)

Apache Spark æ˜¯æœ€æµè¡Œçš„åˆ†å¸ƒå¼æ•°æ®å¤„ç†å’Œåˆ†æå¹³å°ä¹‹ä¸€ã€‚è™½ç„¶å®ƒä¸æœåŠ¡å™¨åœºã€Hadoop å’Œäº‘æŠ€æœ¯ç›¸å…³è”ï¼Œä½†æ‚¨å¯ä»¥åœ¨æ‚¨çš„æœºå™¨ä¸ŠæˆåŠŸå¯åŠ¨å®ƒã€‚åœ¨æœ¬æ¡ç›®ä¸­ï¼Œæ‚¨å°†å­¦ä¹ å‡ ç§é…ç½® Apache Spark å¼€å‘ç¯å¢ƒçš„æ–¹æ³•ã€‚

å‡è®¾

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒåŸºæœ¬ç³»ç»Ÿæ˜¯ Ubuntu æ¡Œé¢ 20.04 LTSã€‚

# ç«èŠ±å£³

ç¬¬ä¸€ç§æ–¹æ³•æ˜¯åœ¨ç»ˆç«¯ä¸­è¿è¡Œ Sparkã€‚è®©æˆ‘ä»¬ä»ä¸‹è½½ Apache Spark å¼€å§‹ã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œä¸‹è½½ã€‚ä¸‹è½½åï¼Œæˆ‘ä»¬å¿…é¡»ç”¨ tar æ‰“å¼€åŒ…ã€‚

```
wget ftp://ftp.task.gda.pl/pub/www/apache/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz
tar zxvf spark-3.0.0-bin-hadoop3.2.tgz
```

Apache Spark æ˜¯ç”¨ Scala å†™çš„ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬éœ€è¦ä¸€ä¸ª Java è™šæ‹Ÿæœº(JVM)ã€‚å¯¹äº Spark 3.0ï¼Œå®ƒå°†æ˜¯ Java 11ã€‚

```
sudo apt install default-jre
```

ç°åœ¨ä½ æ‰€è¦åšçš„å°±æ˜¯è¿›å…¥ bin ç›®å½•å¹¶è¿è¡Œ spark-shell

![](img/bf03744efe6a7a98cdd82b71827796b9.png)

å¦‚æœæ‚¨éœ€è¦ä¸€ä¸ªåº“(ä¾‹å¦‚ï¼Œæ‚¨æƒ³ä» MySQL ä¸‹è½½æ•°æ®ï¼Œå¯¹å…¶è¿›è¡Œå¤„ç†å¹¶ä¿å­˜åˆ°å…¶ä»–åœ°æ–¹)ï¼Œæ‚¨å¯ä»¥æ‰‹åŠ¨é™„åŠ  jar(â€”jars)æˆ–ä» maven å­˜å‚¨åº“ä¸‹è½½å®ƒä»¬(â€” packages)ã€‚

```
./spark-shell --driver-memory 8G --packages mysql:mysql-connector-java:5.1.49
```

# pyspark

åœ¨ spark-shell ä¸­æˆ‘ä»¬ç”¨ Scala ç¼–å†™ï¼Œå¦‚æœä½ æ›´å–œæ¬¢ Pythonï¼Œä½ çš„é€‰æ‹©å°†æ˜¯ PySparkã€‚

ç³»ç»Ÿé‡Œæ²¡æœ‰ Pythonï¼Œæˆ‘ä»¬å°±æ¥ä¸€æ‹›ã€‚æˆ‘ä»¬å°†å®‰è£… pip3ï¼ŒPython å°†ä½œä¸ºä¸€ä¸ªä¾èµ–é¡¹å®‰è£…ğŸ™‚ã€‚

```
sudo apt install python3-pip
```

ä½†äº‹å®è¯æ˜è¿™è¿˜ä¸å¤Ÿã€‚Pyspark æ²¡æœ‰æ‰¾åˆ°å˜é‡ pythonã€‚

```
maciej@ubuntu:~/spark-3.0.0-bin-hadoop3.2/bin$ ./pyspark
env: â€˜pythonâ€™: No such file or directory
```

æˆ‘ä»¬éœ€è¦ä½¿ç”¨ç¯å¢ƒå˜é‡æ¥æŒ‡ç¤º Python ç‰ˆæœ¬ã€‚

```
export PYSPARK_PYTHON=python3
```

ç°åœ¨ pyspark åœ¨ç»ˆç«¯å¯åŠ¨ã€‚

![](img/18ef344cc88fb1a762d80dca2d616e6b.png)

# æœ±åº‡ç‰¹ç¬”è®°æœ¬é‡Œçš„ pyspark

å¤§å¤šæ•°ä½¿ç”¨ Python çš„äººï¼Œè€Œä¸æ˜¯ç»ˆç«¯ï¼Œæ›´å–œæ¬¢ç¬”è®°æœ¬ã€‚æœ€å—æ¬¢è¿çš„æ˜¯ Jupyter ç¬”è®°æœ¬ã€‚è®©æˆ‘ä»¬å®‰è£…å®ƒã€‚æˆ‘ä»¬å°†ä½¿ç”¨ pip3ï¼Œç„¶åæ·»åŠ  */ã€‚local/bin* æ–‡ä»¶å¤¹åˆ°è·¯å¾„ã€‚

```
pip3 install notebook
export PATH=$PATH:~/.local/bin
```

å½“æ‚¨æ·»åŠ ä»¥ä¸‹ç¯å¢ƒå˜é‡æ—¶â€¦

```
export PYSPARK_DRIVER_PYTHON="jupyter"
export PYSPARK_DRIVER_PYTHON_OPTS="notebook"
```

â€¦Jupyter ç¬”è®°æœ¬å°†ä¸ pyspark ä¸€èµ·è‡ªåŠ¨æ¨å‡ºã€‚

![](img/3315eaef76598fc16eac121713342ee8.png)

å¦‚æœéœ€è¦æ·»åŠ åº“ï¼Œè¯·ä½¿ç”¨ä¸‹é¢çš„ç¯å¢ƒå˜é‡ã€‚

```
export PYSPARK_SUBMIT_ARGS='--packages mysql:mysql-connector-java:5.1.49'
```

# Jupter ç¬”è®°æœ¬ä¸­çš„ spylon (scala)

ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æ›´å–œæ¬¢ä½¿ç”¨ Scalaï¼Œå¯ä»¥é€‰æ‹© spylon å†…æ ¸ã€‚å®‰è£…è¿‡ç¨‹å¦‚ä¸‹:

```
pip3 install spylon-kernel
python3 -m spylon_kernel install --user
```

ç„¶åè®¾ç½®ç¯å¢ƒå˜é‡ SPARK_HOMEã€‚

```
export SPARK_HOME=/home/maciej/spark-3.0.0-bin-hadoop3.2
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥è®¿é—®æœ¨æ˜Ÿç¬”è®°æœ¬ä¸­çš„ spylon å†…æ ¸äº†ã€‚

![](img/cd6b360f636b72877a7edcb8f6a49a53.png)![](img/9f098b334729e75c3ef8339b92f98249.png)

å¦‚æœæ‚¨éœ€è¦ç‰¹å®šçš„åŒ…æˆ–é…ç½®ï¼Œè¯·ä½¿ç”¨%%init_spark

![](img/87a0acfedf6f15aa7f92d0d4b0151a40.png)

# IntelliJ ç†å¿µä¸­çš„é¡¹ç›®

ä½ éœ€è¦ Scala æ’ä»¶ã€‚

![](img/7c23c8c82112cbc25c7cb321f14245f6.png)

æˆ‘ä»¬æ­£åœ¨åˆ›å»ºä¸€ä¸ªæ–°çš„ Scala -> sbt é¡¹ç›®ã€‚

![](img/c708498f3d6cd89a001a040839c2bf00.png)

æˆ‘ä»¬é€‰æ‹© Scala 2.12 å’Œ JDKã€‚æˆ‘é€‰æ‹©äº†äºšé©¬é€Š Corretto 11ã€‚

![](img/173e0bc9f767f87c9b7b841a100113c2.png)

è®©æˆ‘ä»¬å°†å¿…è¦çš„åŒ…æ·»åŠ åˆ° build.sbt æ–‡ä»¶ä¸­ã€‚

```
libraryDependencies ++= Seq(
  "org.apache.spark" %% "spark-core" % "3.0.0",
  "org.apache.spark" %% "spark-sql" % "3.0.0"
)
```

åœ¨ sbt åŠ è½½æ›´æ”¹åï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹åœ¨ Spark ä¸­ç¼–å†™åº”ç”¨ç¨‹åºã€‚åœ¨ src/main/scala è·¯å¾„ä¸­åˆ›å»ºä¸€ä¸ªå¯¹è±¡å¹¶å¼€å§‹ç¼–ç ğŸ˜ã€‚

```
import org.apache.spark.sql._

object MyAwesomeApp {
  def main(args: Array[String]) {

    val spark = SparkSession
      .builder
      .appName("MyAwesomeApp")
      .master("local[*]")
      .getOrCreate()

    import spark.implicits._

    val df = Seq(
      ("x", 4),
      ("y", 2),
      ("z", 5)
    ).toDF("some_id", "some_int")

    df.show()
  }
}
```